{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 :ConvNet\n",
    "\n",
    "你所有的网络都来自nn.Module基类：\n",
    "* 在构造函数中，声明你想使用的所有层。\n",
    "* 在forward函数中，你可以定义模型从输入到输出将如何运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTConvNet (\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (fc1): Linear (320 -> 50)\n",
      "  (fc2): Linear (50 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MNISTConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 这是你实例化所有模块的地方\n",
    "        # 你可以稍后使用你在此给出的相同名称访问它们\n",
    "        super(MNISTConvNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,5)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(10,20,5)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "        \n",
    "    #这是 forward 函数，它定义了我们在这里只接受一个输入的网络结构，\n",
    "    #如果你远离，可以随意使用更多的输入。\n",
    "    def forward(self,input):\n",
    "        x = self.pool1(F.relu(self.conv1(input)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # 在你的模型定义中, 你可以疯狂地使用任意的python代码来定义你的模型结构,\n",
    "        # 这些都是完全合法的, 并且会被autograd正确处理:\n",
    "        # if x.gt(0) > x.numel() / 2:\n",
    "        #      ...\n",
    "        #\n",
    "        # 你甚至可以做一个循环, 并重复使用相同的模块, 模块内部的模块不再\n",
    "        # 处于临时状态, 所以你可以在 forward 时多次使用它们.\n",
    "        # while x.norm(2) < 10:\n",
    "        #    x = self.conv1(x)\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "net = MNISTConvNet()\n",
    "print(net)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个包含随机数据的单样本的mini-batch,并通过ConvNet发送样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1,1,28,28))\n",
    "out = net(input)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.3789\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#定义一个虚拟目标标签，并使用损失函数计算error.\n",
    "target = Variable(torch.LongTensor([3]))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "err = loss_fn(out,target)\n",
    "err.backward()\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet 的 out 是一个 Variable. 我们使用它来计算损失, 计算结果 err 也是一个 Variable. 在 err 上调用 .backward 将会通过 ConvNet 将梯度传播到它的权重."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.1764  0.0057  0.0445  0.0061  0.0376\n",
      "  0.0031  0.1113 -0.1774 -0.1272  0.0410\n",
      " -0.0143  0.0059  0.0486 -0.1938  0.0335\n",
      " -0.1076  0.0296 -0.0670 -0.1987  0.0958\n",
      " -0.1837  0.1265 -0.1571 -0.1050 -0.1847\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0194 -0.1014 -0.1065 -0.0596  0.0985\n",
      " -0.1978  0.0089  0.0193 -0.1824 -0.1141\n",
      "  0.1127 -0.1162  0.0454 -0.1359  0.1376\n",
      "  0.0678 -0.0188  0.1989 -0.0285  0.0114\n",
      "  0.0099 -0.0934  0.0677  0.1837 -0.1324\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -0.0025  0.0996  0.0791 -0.0920 -0.1033\n",
      "  0.0933  0.1085  0.1291  0.0432  0.1915\n",
      " -0.0939 -0.1067 -0.1400  0.1442  0.0277\n",
      " -0.1524  0.0396 -0.0022  0.1047  0.0804\n",
      " -0.0738  0.1718  0.0652  0.0419  0.1983\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.1867 -0.0591  0.1298  0.0287  0.0338\n",
      " -0.0561  0.0323 -0.0950  0.0922  0.0471\n",
      "  0.1360  0.1524 -0.0913  0.1020  0.0365\n",
      "  0.0740  0.0190 -0.1210 -0.0914 -0.0836\n",
      "  0.1328  0.0521  0.0383 -0.0060  0.0320\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.1761  0.0099  0.1225 -0.1637  0.1168\n",
      "  0.0729  0.0948 -0.1005  0.1350  0.1883\n",
      " -0.0871  0.0838 -0.0798  0.1897  0.1648\n",
      "  0.0546 -0.1549 -0.0470  0.1580 -0.1975\n",
      "  0.1930 -0.1718  0.1292 -0.0914 -0.0722\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.0295 -0.1470 -0.0858  0.0253 -0.0191\n",
      " -0.0782  0.0972 -0.1065  0.0546 -0.0338\n",
      "  0.1175  0.0693 -0.0864 -0.0826 -0.0074\n",
      " -0.1207  0.0809  0.1989  0.1054  0.1312\n",
      " -0.1411  0.1539  0.1394 -0.0081  0.0891\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.0539  0.0106  0.1819 -0.0794  0.0103\n",
      "  0.0562 -0.1796  0.0858  0.1793  0.0403\n",
      "  0.1134  0.1368 -0.0331  0.1050  0.1842\n",
      " -0.1361  0.0098 -0.1687  0.0176 -0.1996\n",
      " -0.1503  0.1614  0.1927  0.0818 -0.1102\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "  0.0138  0.1888  0.1089  0.0238  0.1638\n",
      "  0.0283 -0.1519  0.0068 -0.0192  0.1294\n",
      "  0.0031 -0.1310 -0.0669  0.0171 -0.0882\n",
      " -0.1738  0.1889  0.1642  0.0946  0.0966\n",
      "  0.0111 -0.1548  0.1694  0.0041 -0.1605\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "  0.1388  0.1557  0.0622 -0.0826 -0.1756\n",
      " -0.0793 -0.1437 -0.0454  0.1266 -0.1124\n",
      " -0.0185  0.0238  0.0198  0.1822  0.0004\n",
      "  0.0616  0.0295 -0.0532  0.0515  0.0154\n",
      "  0.0532 -0.0749 -0.0301 -0.1609  0.0702\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      "  0.1189 -0.0762  0.1701  0.1391 -0.0253\n",
      "  0.1629  0.0207  0.1833  0.1567  0.0086\n",
      "  0.0479  0.1827 -0.1570  0.1372 -0.1055\n",
      " -0.0601  0.0148 -0.1163  0.1602 -0.1220\n",
      "  0.1859 -0.1594  0.0936  0.0965 -0.1758\n",
      "[torch.FloatTensor of size 10x1x5x5]\n",
      "\n",
      "torch.Size([10, 1, 5, 5])\n",
      "1.7812725029562049\n",
      "0.10628808400648986\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight)\n",
    "print(net.conv1.weight.grad.size())\n",
    "print(net.conv1.weight.data.norm())  # norm of the weight\n",
    "print(net.conv1.weight.grad.data.norm())  # norm of the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and Backward Function Hooks\n",
    "\n",
    "你可以在一个 Module 或一个 Variable 上注册一个函数. hook 可以是 forward hook 也可以是一个 backward hook. 当 forward 被执行时 forward hook 将会被执行. backward hook 将在 backward 阶段被执行."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward_hook\n",
    "\n",
    "def printnorm(self,input,output):\n",
    "    #输入是打包输入的tuple\n",
    "    #输出是一个 Variable。 output.data 是我们感兴趣的Tensor\n",
    "    print('Inside' + self.__class__.__name__ + 'forward')\n",
    "    print('')\n",
    "    print('input : ' + type(input))\n",
    "    print('input[0]: ', type(input[0]))\n",
    "    print('output: ', type(output))\n",
    "    print('')\n",
    "    print('input size:', input[0].size())\n",
    "    print('output size:', output.data.size())\n",
    "    print('output norm:', output.data.norm())\n",
    "\n",
    "    \n",
    "net.conv2.register_forward_hook(printnorm)\n",
    "\n",
    "out = net(input)  \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printgradnorm(self, grad_input, grad_output):\n",
    "    print('Inside ' + self.__class__.__name__ + ' backward')\n",
    "    print('Inside class:' + self.__class__.__name__)\n",
    "    print('')\n",
    "    print('grad_input: ', type(grad_input))\n",
    "    print('grad_input[0]: ', type(grad_input[0]))\n",
    "    print('grad_output: ', type(grad_output))\n",
    "    print('grad_output[0]: ', type(grad_output[0]))\n",
    "    print('')\n",
    "    print('grad_input size:', grad_input[0].size())\n",
    "    print('grad_output size:', grad_output[0].size())\n",
    "    print('grad_input norm:', grad_input[0].data.norm())\n",
    "\n",
    "\n",
    "net.conv2.register_backward_hook(printgradnorm)\n",
    "\n",
    "out = net(input)\n",
    "err = loss_fn(out, target)\n",
    "err.backward()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
